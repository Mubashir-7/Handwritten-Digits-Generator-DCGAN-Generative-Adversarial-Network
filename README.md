🖊️ Handwritten Digits Generator - DCGAN
📌 Project Overview

This project implements a Deep Convolutional Generative Adversarial Network (DCGAN) to generate synthetic handwritten digits similar to the MNIST dataset.
The model consists of two adversarial networks:

Generator: Learns to create realistic digit-like images from random noise.

Discriminator: Learns to distinguish between real MNIST images and synthetic (fake) images generated by the Generator.

Through adversarial training, the Generator improves to fool the Discriminator, resulting in realistic handwritten digit images.

🎯 Objectives

Learn and implement GAN fundamentals using DCGAN architecture.

Train a generator to produce high-quality handwritten digits.

Demonstrate adversarial learning where two networks compete and improve together.

🏗️ System Architecture

The workflow is as follows:

Input:

Random noise (latent vector z) is fed into the Generator.

Real handwritten images (MNIST dataset) are used as ground truth.

Generator (G):

Transforms noise into synthetic images using transposed convolutional layers (ConvTranspose2D).

Discriminator (D):

A convolutional neural network that classifies images as Real (1) or Fake (0).

Adversarial Training:

Generator tries to fool the Discriminator.

Discriminator tries to correctly classify real vs fake.

The system reaches equilibrium when Generator produces digit images indistinguishable from real ones.

📊 Dataset

MNIST Dataset (60,000 training, 10,000 test images).

Images are grayscale, size 28×28 pixels.

Preprocessing: Normalization to range [-1, 1] for stable GAN training.

⚙️ Technical Details
Generator Network (G):

Input: Random noise vector (z ∼ N(0,1))

Layers:

Fully connected + Reshape

ConvTranspose2D + BatchNorm + ReLU (upsampling layers)

Final ConvTranspose2D + Tanh (outputs image of shape 28×28×1)

Discriminator Network (D):

Input: Real or fake image

Layers:

Convolutional layers with LeakyReLU

Dropout for regularization

Final Dense layer + Sigmoid (binary classification: real/fake)

Loss Functions:

Binary Cross Entropy (BCE) for both Generator & Discriminator.

Training objective:

Discriminator: Maximize log(D(x)) + log(1 - D(G(z)))

Generator: Minimize log(1 - D(G(z))) (or equivalently maximize log(D(G(z))))

Optimizer:

Adam Optimizer (lr=0.0002, β1=0.5) for both G and D.

🚀 Training Procedure

Initialize Generator (G) and Discriminator (D).

For each epoch:

Train Discriminator on:

Real images labeled as 1

Fake images from Generator labeled as 0

Train Generator to fool Discriminator by generating images labeled as 1.

Repeat until Generator produces realistic digit-like images.

📈 Results & Evaluation

Generated images improve as training progresses.

Discriminator accuracy decreases as Generator gets better.

Visualizations of generated digits are saved after each epoch.

💻 Tech Stack

Language: Python

Frameworks: PyTorch / TensorFlow (depending on implementation)

Libraries: NumPy, Matplotlib, tqdm
